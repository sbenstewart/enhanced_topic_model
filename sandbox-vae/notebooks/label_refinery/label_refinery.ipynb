{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%matplotlib inline\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import h5py\n",
    "import keras\n",
    "import pandas as pd\n",
    "import math\n",
    "import joblib\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from fuel.datasets.svhn import SVHN\n",
    "from IPython.display import display\n",
    "\n",
    "from keras.layers import (Input, Dense, Lambda, Flatten, Reshape, BatchNormalization, \n",
    "                          Activation, Dropout, Conv2D, Conv2DTranspose,\n",
    "                          Concatenate, Add, Multiply)\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import RandomUniform\n",
    "from keras.optimizers import RMSprop, Adam, SGD\n",
    "from keras.models import Model\n",
    "from keras import metrics\n",
    "from keras import backend as K\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.datasets import mnist\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from resnet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows, img_cols, img_chns = 32, 32, 3\n",
    "original_img_size = (img_rows, img_cols, img_chns)\n",
    "num_classes = 10\n",
    "learning_rate = float(os.environ.get('LEARNING_RATE', 0.001))\n",
    "decay = float(os.environ.get('DECAY', 0.0))\n",
    "batch_size = int(os.environ.get('BATCH_SIZE', 250))\n",
    "epochs = int(os.environ.get('EPOCHS', 100))\n",
    "run_num = int(os.environ.get('RUN_NUM', 0))\n",
    "use_preprocessing = int(os.environ.get('USE_PREPROCESSING', 1))\n",
    "\n",
    "file_prefix = 'results_preproc_%d_run_%d_' % (use_preprocessing, run_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_svhn(split):\n",
    "    f = SVHN(which_format=2, which_sets=(split,), load_in_memory=True)\n",
    "    f.load()\n",
    "    X_dataset, y_dataset = f.data_sources\n",
    "    \n",
    "    X_dataset, y_dataset = np.moveaxis(X_dataset, 1, 3), y_dataset\n",
    "    X_dataset = X_dataset / 255.\n",
    "    y_dataset = keras.utils.to_categorical(y_dataset, 10)\n",
    "    print (\"%s - DType X=%s, y=%s\" % (split, X_dataset.dtype, y_dataset.dtype))\n",
    "    print (\"%s - Shape X=%s, y=%s\" % (split, X_dataset.shape, y_dataset.shape))\n",
    "    \n",
    "    return X_dataset, y_dataset\n",
    "\n",
    "\n",
    "X_train_raw, y_train_raw = get_svhn('train')\n",
    "validation_index = int(len(X_train_raw) * 0.85)\n",
    "X_validation, y_validation = X_train_raw[validation_index:], y_train_raw[validation_index:]\n",
    "X_train, y_train = X_train_raw[:validation_index], y_train_raw[:validation_index]\n",
    "X_test, y_test = get_svhn('test')\n",
    "\n",
    "print(\"raw\", len(X_train_raw), len(y_train_raw))\n",
    "print(\"validation\", len(X_validation), len(y_validation))\n",
    "print(\"train\", len(X_train), len(y_train))\n",
    "print(\"test\", len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment data\n",
    "if use_preprocessing:\n",
    "    datagen = ImageDataGenerator(\n",
    "        zoom_range=0.10,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        fill_mode='constant',\n",
    "        rotation_range=10)\n",
    "    datagen.fit(X_train)\n",
    "          \n",
    "    X_aug = [X_train]\n",
    "    y_aug = [y_train]\n",
    "    for i in range(2):\n",
    "        X_gen, y_gen = next(datagen.flow(X_train,  y_train, batch_size=len(X_train)))\n",
    "        X_aug.append(X_gen)\n",
    "        y_aug.append(y_gen)\n",
    "        \n",
    "    X_train = np.concatenate(tuple(X_aug))\n",
    "    y_train = np.concatenate(tuple(y_aug))\n",
    "    del X_aug, y_aug, X_gen, y_gen\n",
    "    print(X_train.shape, X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_grid(dataset, digit_size=32, grid_size=5, seed=None):\n",
    "    # Display some digits to figure out what's going on\n",
    "    figure = np.zeros((digit_size * grid_size, digit_size * grid_size, 3))\n",
    "   \n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            digit = dataset[np.random.randint(len(dataset))]\n",
    "            d_x, d_y = i * digit_size, j * digit_size\n",
    "            figure[d_x:d_x + digit_size, d_y:d_y + digit_size, :] = digit.astype(float)\n",
    "            \n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(figure)\n",
    "    plt.show()\n",
    "\n",
    "display_grid(X_train, seed=0)\n",
    "display_grid(X_test, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    x_input = Input(batch_shape=(None,) + original_img_size)\n",
    "    resnet_model = ResNet50(weights=None, pooling='avg', input_shape=original_img_size, include_top=False)\n",
    "    model_out = resnet_model(x_input)\n",
    "    out = Dense(num_classes, activation='softmax', name='fc10')(model_out)\n",
    "    model = Model(x_input, out, name='myresent50')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, y_train_vals):\n",
    "    optimizer = Adam(lr=learning_rate, decay=decay)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    early_stopping = keras.callbacks.EarlyStopping('val_acc', min_delta=0.1, patience=20)\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.1, patience=10, min_lr=0.01 * learning_rate)\n",
    "    \n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    "    if 'CMDLINE' not in os.environ:\n",
    "        callbacks += [TQDMNotebookCallback()]\n",
    "        \n",
    "    history = model.fit(\n",
    "         X_train, y_train_vals,\n",
    "         batch_size=batch_size,\n",
    "         epochs=epochs,\n",
    "         callbacks=callbacks,\n",
    "         validation_data=(X_validation, y_validation),\n",
    "         verbose=0\n",
    "     )\n",
    "    \n",
    "    done = time.time()\n",
    "    elapsed = done - start\n",
    "    print(\"Elapsed: \", elapsed)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Testing\n",
    "# model = make_model()\n",
    "# model.summary()\n",
    "# epochs=1\n",
    "# model, history = train_model(model, y_train)\n",
    "# history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_train_predict = y_train\n",
    "for i in range(3):\n",
    "    print(\"Iteration\", i)\n",
    "    model = make_model()\n",
    "    model.summary()\n",
    "    model, history = train_model(model, y_train_predict)\n",
    "    \n",
    "    df = pd.DataFrame(history.history)\n",
    "    display(df.describe(percentiles=[0.25 * i for i in range(4)] + [0.95, 0.99]))\n",
    "    df.plot(figsize=(8, 6))\n",
    "    df.to_csv(file_prefix + ('history_iter%d' % i) + '.csv', index=False)\n",
    "    with open(file_prefix + 'vars.txt', 'w') as f:\n",
    "        f.write(str(locals()))\n",
    "    \n",
    "    y_train_predict = model.predict(X_train)\n",
    "    y_train_predict\n",
    "    \n",
    "    test_results = model.evaluate(X_test, y_test)\n",
    "    print(test_results)\n",
    "    with open('allresults.csv', 'a') as f:\n",
    "        line = ','.join([str(use_preprocessing), str(run_num), str(i)] + [str(x) for x in test_results])\n",
    "        f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "locals()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
